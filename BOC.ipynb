{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(images_paths, labels_paths, valid_labels):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    no_label_counter = 0\n",
    "    unidentified_counter = 0\n",
    "\n",
    "    for images_path, labels_path in zip(images_paths, labels_paths):\n",
    "\n",
    "        # Load labels from csv file\n",
    "        labels_df = pd.read_csv(labels_path, sep=';')\n",
    "        labels_df['Filename'] = [file[:-3] + 'png' for file in labels_df['Filename']]\n",
    "\n",
    "        for filename in os.listdir(images_path):\n",
    "            # Load the image using PIL\n",
    "            img_path = os.path.join(images_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # Crop the image to the size of the spectrogram\n",
    "            left, upper, right, lower = 55, 36, 389, 252\n",
    "            img = img.crop((left, upper, right, lower))\n",
    "\n",
    "            # Convert image to numpy array and normalize\n",
    "            img_array = np.array(img)[:, :, :3] / 255.0\n",
    "\n",
    "            # Extract class label from the CSV file based on the image filename\n",
    "            label_row = labels_df.loc[labels_df['Filename'] == filename]\n",
    "            \n",
    "            if label_row.empty:\n",
    "                no_label_counter += 1\n",
    "                continue\n",
    "\n",
    "            label = label_row['Species'].values[0]\n",
    "\n",
    "            if label not in valid_labels:\n",
    "                unidentified_counter += 1\n",
    "                continue\n",
    "                \n",
    "            labels.append(label)\n",
    "            images.append(img_array)\n",
    "                \n",
    "\n",
    "    if no_label_counter:\n",
    "        print(f'Label not found for {no_label_counter} images : Images will not be used.')\n",
    "    if unidentified_counter:\n",
    "        print(f'Bat unidentified for {unidentified_counter} images : Images will not be used.')\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folders = ['./Data/dataset1', './Data/dataset2', './Data/dataset3']\n",
    "labels_paths = ['./Data/dataset1_classified.csv', './Data/dataset2_classified.csv', './Data/dataset3_classified.csv']\n",
    "\n",
    "valid_labels = ['Bartfledermaus', 'Bechsteinfledermaus', 'Fransenfledermaus',\n",
    "                'Gro√üe Hufeisennase', 'Hufeisennase', 'Mausohr',\n",
    "                'Langohrfledermaus', 'Wasserfledermaus', 'Wimperfledermaus']\n",
    "\n",
    "images, labels = load_images(images_folders, labels_paths, valid_labels)\n",
    "print('Images shape: ', images.shape)\n",
    "print('Labels shape: ', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Data Augmentation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 12\n",
    "indices_to_visualize = random.sample(range(len(images)), num_samples)\n",
    "\n",
    "num_cols = 4\n",
    "num_rows = (num_samples + num_cols - 1) // num_cols\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "for i, index in enumerate(indices_to_visualize):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "\n",
    "    axes[row, col].imshow(images[index])\n",
    "    axes[row, col].set_title(f'Class: {labels[index]}')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "sns.countplot(x=labels)\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of images: ', np.mean(images))\n",
    "print('Std deviation of images: ', np.std(images))\n",
    "\n",
    "empty_string_indices = labels == ''\n",
    "print('Missing values in labels: ', np.sum(empty_string_indices))\n",
    "print('Missing values in images: ', np.isnan(images).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(encoded_labels),\n",
    "                                                  y=encoded_labels)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build the model with hyperparameters\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(hp.Int('conv1_units', min_value=16, max_value=64, step=16), (3, 3), activation='relu', input_shape=(216, 334, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(hp.Int('conv2_units', min_value=32, max_value=128, step=32), (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(hp.Int('conv3_units', min_value=64, max_value=256, step=64), (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(hp.Int('dense_units', min_value=64, max_value=512, step=64), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')), \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(build_model, objective='val_accuracy', max_trials=20, directory='./cnn-model/tuner_results', project_name='cnn_tuner',\n",
    "                     overwrite=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint callback\n",
    "checkpoint_path = './cnn-model/model_checkpoint.keras'\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, epochs=25, validation_split=0.2, class_weight=class_weights_dict, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('./cnn-model/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "cnn_model = tf.keras.models.load_model('./cnn-model/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model again\n",
    "history = cnn_model.fit(X_train, y_train, epochs=50, class_weight=class_weights_dict, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "cnn_model.save('./cnn-model/final_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "cnn_final_model = tf.keras.models.load_model('./cnn-model/final_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = cnn_final_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = label_encoder.classes_\n",
    "\n",
    "y_pred_probabilities = cnn_final_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_test_int = np.argmax(tf.keras.utils.to_categorical(y_test), axis=1)\n",
    "\n",
    "# Extract unique classes present in the test dataset\n",
    "unique_classes_in_test = np.unique(y_test_int)\n",
    "\n",
    "# Filter the all_classes list to include only those present in the test dataset\n",
    "target_classes = [all_classes[i] for i in unique_classes_in_test]\n",
    "\n",
    "# Convert the integer labels to string labels using the label_encoder\n",
    "y_test_int_str = label_encoder.inverse_transform(y_test_int)\n",
    "y_pred_str = label_encoder.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "confusion_m = confusion_matrix(y_test_int, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_m, annot=True, fmt='d', cmap='Greens', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test_int_str, y_pred_str, target_names=target_classes, zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
